{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Naive Bayes Classification**<font color='violet'></br>\n",
    "<br>Naive bayes classification is an effective algorithm to predict the category/class of data points based on a training data set, and it works on Bayes theorem of probability to predict the class of unknown data sets. Utilizing this algorithm, we assume independency among predictors which is a strongly simplifying yet affective assumption.</br>\n",
    "<br>In this project we have been given a training data set from which we have to evaluate each word's probability of appearance in each class based on its experienced frequency. </br>\n",
    "<br>Steps:</br></font><font color='blue'>\n",
    "1. Dealing with text data:\n",
    "    1.1. Normalizing: \n",
    "    1.2. Tokenizing: using white-space and characters, we split each comment to a list\n",
    "    1.3. Stemming and lemmatization:\n",
    "    With stemming we cut the word to its root by usually ommitting the last alphabet letter which was further added to it to maybe form a plural word. Stemming sometimes tends to reduce the accuracy of the model and precision performance but increses recall performance.\n",
    "    Lemmatization is a method to accurately identify a word's root using its part of speech tagger and vocabulary words. This may take up some disk space and require a lot of processing time but it will improve acuracy and precision.  \n",
    "2. Finding word counts in training data set\n",
    "3. Calculating probabilities and making predictions\n",
    "</font>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from collections import deque\n",
    "from parsivar import FindStems\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import datetime\n",
    "import secrets\n",
    "import csv \n",
    "import nltk                                         #Natural language processing tool-kit\n",
    "\n",
    "from nltk.stem import PorterStemmer                 # Stemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') #didnt actually use it\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\n",
    "from gensim.models import Word2Vec                                   #For Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def import_data(filename):\n",
    "    table=[]\n",
    "    counter=0\n",
    "    with open(filename,encoding=\"utf8\") as csvfile:\n",
    "        sreader=pd.read_csv(csvfile)\n",
    "    csvfile.close()\n",
    "    return sreader\n",
    "\n",
    "data=import_data(\"comment_train.csv\")\n",
    "test=import_data(\"comment_test.csv\")\n",
    "\n",
    "persian_stopwords=open(\"persian.txt\",encoding=\"utf8\")\n",
    "stopwords_list=deque([])\n",
    "for line in persian_stopwords:\n",
    "    stopwords_list.append(line.strip())\n",
    "persian_stopwords.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>وری گود</td>\n",
       "      <td>تازه خریدم یه مدت کار بکنه مشخص میشه کیفیت قطعاتش</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>زیاد مناسب نیست رنگ پس میده یه وقتایی موقع نوشتن</td>\n",
       "      <td>با این قیمت گزینه های بهتری هم میشه گرفت.\\nروا...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>پنکه گوشی</td>\n",
       "      <td>خیلی عالیه، فقط کاش از اون سمتش میشد به پاوربا...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>دستگاه خیلی ضعیف</td>\n",
       "      <td>من این فیس براس چند روز یپش به دستم رسید و الا...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>عالی و بیست</td>\n",
       "      <td>بنده یه هارد اکسترنال دارم که کابل فابریکش سال...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>بسیار کوچیک</td>\n",
       "      <td>طراحیش قشنگه ولی  داخل عکس خیلی بزرگتر ب چشم م...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>لامپ چینی</td>\n",
       "      <td>این لامپ چینی هستتش کیفیت پایین . نور کم و فاق...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>خوب بود</td>\n",
       "      <td>در کل از این خریدم راضی هستم و به تناسب قیمتش ...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>کیفیت خوبی داره</td>\n",
       "      <td>تازع نصبش کردم-سرعت انتقال و نصب بازی روش عالی...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>از خریدم پشیمونم</td>\n",
       "      <td>نسبت به باطری اصلی فقط 60% کار میکنه\\nالان دو ...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                                             وری گود   \n",
       "1    زیاد مناسب نیست رنگ پس میده یه وقتایی موقع نوشتن   \n",
       "2                                           پنکه گوشی   \n",
       "3                                    دستگاه خیلی ضعیف   \n",
       "4                                         عالی و بیست   \n",
       "..                                                ...   \n",
       "795                                       بسیار کوچیک   \n",
       "796                                         لامپ چینی   \n",
       "797                                           خوب بود   \n",
       "798                                   کیفیت خوبی داره   \n",
       "799                                  از خریدم پشیمونم   \n",
       "\n",
       "                                               comment        recommend  \n",
       "0    تازه خریدم یه مدت کار بکنه مشخص میشه کیفیت قطعاتش      recommended  \n",
       "1    با این قیمت گزینه های بهتری هم میشه گرفت.\\nروا...  not_recommended  \n",
       "2    خیلی عالیه، فقط کاش از اون سمتش میشد به پاوربا...      recommended  \n",
       "3    من این فیس براس چند روز یپش به دستم رسید و الا...  not_recommended  \n",
       "4    بنده یه هارد اکسترنال دارم که کابل فابریکش سال...      recommended  \n",
       "..                                                 ...              ...  \n",
       "795  طراحیش قشنگه ولی  داخل عکس خیلی بزرگتر ب چشم م...  not_recommended  \n",
       "796  این لامپ چینی هستتش کیفیت پایین . نور کم و فاق...  not_recommended  \n",
       "797  در کل از این خریدم راضی هستم و به تناسب قیمتش ...      recommended  \n",
       "798  تازع نصبش کردم-سرعت انتقال و نصب بازی روش عالی...      recommended  \n",
       "799  نسبت به باطری اصلی فقط 60% کار میکنه\\nالان دو ...  not_recommended  \n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>زیبا اما کم دوام</td>\n",
       "      <td>با وجود سابقه خوبی که از برند ایرانی نهرین سرا...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>بسیار عالی</td>\n",
       "      <td>بسیار عالی</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>سلام</td>\n",
       "      <td>من الان ۳ هفته هست استفاده میکنم\\nبرای کسایی ک...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>به درد نمیخورهههه</td>\n",
       "      <td>عمرش کمه تا یه هفته بیشتر نمیشه استفاده کرد یا...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>کلمن آب</td>\n",
       "      <td>فکر کنین کلمن بخرین با ذوق. کلی پولشو بدین. به...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>جنسش عالیه</td>\n",
       "      <td>خیلی جنس پارچش نرم ولطیفه خیلیم جنسش خوبه اما ...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>خرید محصول</td>\n",
       "      <td>سلام.واقعا فکر نمی کردم به این راحتی اصلاح کنم...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>تعریف</td>\n",
       "      <td>من از دیجی کالا خریدم خیلی زود دستم رسید،زیبا،...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>اصلا چای ماچا نیسش</td>\n",
       "      <td>یا شرکت نمیدونسته چای ماچا امپریال چیه یا واقع...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>یک هدست بدون سیم عالی</td>\n",
       "      <td>این هدست را چند هفته قبل از سایت دیجی کالا خری...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  \\\n",
       "0          زیبا اما کم دوام   \n",
       "1                بسیار عالی   \n",
       "2                      سلام   \n",
       "3         به درد نمیخورهههه   \n",
       "4                   کلمن آب   \n",
       "...                     ...   \n",
       "5995             جنسش عالیه   \n",
       "5996             خرید محصول   \n",
       "5997                  تعریف   \n",
       "5998     اصلا چای ماچا نیسش   \n",
       "5999  یک هدست بدون سیم عالی   \n",
       "\n",
       "                                                comment        recommend  \n",
       "0     با وجود سابقه خوبی که از برند ایرانی نهرین سرا...  not_recommended  \n",
       "1                                            بسیار عالی      recommended  \n",
       "2     من الان ۳ هفته هست استفاده میکنم\\nبرای کسایی ک...  not_recommended  \n",
       "3     عمرش کمه تا یه هفته بیشتر نمیشه استفاده کرد یا...  not_recommended  \n",
       "4     فکر کنین کلمن بخرین با ذوق. کلی پولشو بدین. به...  not_recommended  \n",
       "...                                                 ...              ...  \n",
       "5995  خیلی جنس پارچش نرم ولطیفه خیلیم جنسش خوبه اما ...      recommended  \n",
       "5996  سلام.واقعا فکر نمی کردم به این راحتی اصلاح کنم...      recommended  \n",
       "5997  من از دیجی کالا خریدم خیلی زود دستم رسید،زیبا،...      recommended  \n",
       "5998  یا شرکت نمیدونسته چای ماچا امپریال چیه یا واقع...  not_recommended  \n",
       "5999  این هدست را چند هفته قبل از سایت دیجی کالا خری...      recommended  \n",
       "\n",
       "[6000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_data(data):\n",
    "    normalizer=Normalizer()\n",
    "    lemmatizer=Lemmatizer()\n",
    "    stemmer=Stemmer()\n",
    "    stemm=FindStems()\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(3):\n",
    "            normalizer.normalize(data.iloc[i,j])\n",
    "\n",
    "    comments=deque([])\n",
    "    titles=deque([])\n",
    "    bag_of_words_recom=deque([])\n",
    "    bag_of_words_nrecom=deque([])\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        title_list=word_tokenize(data.iloc[i,0])\n",
    "        comment_list=word_tokenize(data.iloc[i,1])\n",
    "\n",
    "        list_t=[stemm.convert_to_stem(x) for x in title_list if not x in stopwords_list]\n",
    "        list_c=[stemm.convert_to_stem(x) for x in comment_list if not x in stopwords_list]\n",
    "        title_list=deque(list_t)\n",
    "        comment_list=deque(list_c)\n",
    "        comments.append(comment_list)\n",
    "        titles.append(title_list)\n",
    "        comment_list+=title_list\n",
    "        if data.loc[i,'recommend']=='recommended': \n",
    "            bag_of_words_recom+=comment_list\n",
    "        else:\n",
    "            bag_of_words_nrecom+=comment_list\n",
    "            \n",
    "    return bag_of_words_recom,bag_of_words_nrecom\n",
    "\n",
    "def unpreprocessed_data(data):    \n",
    "    comments=deque([])\n",
    "    titles=deque([])\n",
    "    bag_of_words_recom=deque([])\n",
    "    bag_of_words_nrecom=deque([])\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        title_list=word_tokenize(data.iloc[i,0])\n",
    "        comment_list=word_tokenize(data.iloc[i,1])\n",
    "        comments.append(comment_list)\n",
    "        titles.append(title_list)\n",
    "        comment_list+=title_list\n",
    "        if data.loc[i,'recommend']=='recommended': \n",
    "            bag_of_words_recom+=comment_list\n",
    "        else:\n",
    "            bag_of_words_nrecom+=comment_list\n",
    "\n",
    "    return bag_of_words_recom,bag_of_words_nrecom\n",
    "\n",
    "bag_of_words_recom,bag_of_words_nrecom=preprocessed_data(data)\n",
    "bag_of_words_recom1,bag_of_words_nrecom1=unpreprocessed_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recommended_no=data[data.recommend=='recommended'].shape[0]\n",
    "train_nrecommended_no=data[data.recommend=='not_recommended'].shape[0]\n",
    "probab_recom=train_recommended_no/data.shape[0]\n",
    "probab_nrecom=train_nrecommended_no/data.shape[0]\n",
    "\n",
    "#preprocessed\n",
    "total_words_recom=len(bag_of_words_recom)\n",
    "total_words_nrecom=len(bag_of_words_nrecom)\n",
    "\n",
    "occurences_recom=list(set(list(bag_of_words_recom)))\n",
    "occurences_nrecom=list(set(list(bag_of_words_nrecom)))\n",
    "\n",
    "repeats_recom=[]\n",
    "repeats_nrecom=[]\n",
    "\n",
    "for i in range(len(occurences_recom)):\n",
    "    repeats_recom.append(bag_of_words_recom.count(occurences_recom[i]))\n",
    "\n",
    "for i in range(len(occurences_nrecom)):\n",
    "    repeats_nrecom.append(bag_of_words_nrecom.count(occurences_nrecom[i]))\n",
    "    \n",
    "probab_words_recom=[x/total_words_recom for x in repeats_recom]\n",
    "probab_words_nrecom=[x/total_words_nrecom for x in repeats_nrecom]\n",
    "\n",
    "#unpreprocessed\n",
    "total_words_recom1=len(bag_of_words_recom1)\n",
    "total_words_nrecom1=len(bag_of_words_nrecom1)\n",
    "\n",
    "occurences_recom1=list(set(list(bag_of_words_recom1)))\n",
    "occurences_nrecom1=list(set(list(bag_of_words_nrecom1)))\n",
    "\n",
    "repeats_recom1=[]\n",
    "repeats_nrecom1=[]\n",
    "\n",
    "for i in range(len(occurences_recom1)):\n",
    "    repeats_recom1.append(bag_of_words_recom1.count(occurences_recom1[i]))\n",
    "\n",
    "for i in range(len(occurences_nrecom1)):\n",
    "    repeats_nrecom1.append(bag_of_words_nrecom1.count(occurences_nrecom1[i]))\n",
    "    \n",
    "probab_words_recom1=[x/total_words_recom1 for x in repeats_recom1]\n",
    "probab_words_nrecom1=[x/total_words_nrecom1 for x in repeats_nrecom1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font color='green'>Additive Smoothing</br></font><font color='orange'>\n",
    "<br>When a word's frequency belonging to a class in the training data set is zero, our model will estimate the probability of the word's appearance in the other class to be zero and it will wipe out other information on the comment's classification derived from the probabilites of other existing words. So we will apply a small correction and assign a low value to the probability of the word so that no word's probability will be zero. Each time we do this, we add the mentioned value to the total frequency of the words to regularize the whole calculation.</br></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed & smoothed\n",
    "def predict_recom(word_bag_recom,word_bag_nrecom,repeats_recom,repeats_nrecom,bag):\n",
    "    probab_recom=calc_probab(word_bag_recom,repeats_recom,bag)\n",
    "    probab_nrecom=calc_probab(word_bag_nrecom,repeats_nrecom,bag)\n",
    "    \n",
    "    if probab_recom>=probab_nrecom:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def calc_probab(trained_bag,repeats,bag):\n",
    "    n=0\n",
    "    probab=1\n",
    "    tot=len(trained_bag)\n",
    "    words_probab=[]\n",
    "    for word in bag:\n",
    "        exists=trained_bag.count(word)\n",
    "        if exists:\n",
    "            idx=trained_bag.index(word)\n",
    "            words_probab.append(repeats[idx])\n",
    "        else:\n",
    "            words_probab.append(0.5)\n",
    "            tot=tot+0.5\n",
    "            \n",
    "    words_probab=[x/tot for x in words_probab]\n",
    "    return np.prod(words_probab)\n",
    "def naive_bayes_preprocessed_smoothed(test):\n",
    "    normalizer=Normalizer()\n",
    "    lemmatizer=Lemmatizer()\n",
    "    stemmer=Stemmer()\n",
    "    stemm=FindStems()\n",
    "    for i in range(test.shape[0]):\n",
    "        for j in range(3):\n",
    "            normalizer.normalize(test.iloc[i,j])\n",
    "    naive_bayes_predicts=[]\n",
    "    comments=deque([])\n",
    "    titles=deque([])\n",
    "    bag=deque([])\n",
    "    for i in range(test.shape[0]):\n",
    "        title_list=word_tokenize(test.iloc[i,0])\n",
    "        comment_list=word_tokenize(test.iloc[i,1])\n",
    "        list_t=[stemm.convert_to_stem(x) for x in title_list if not x in stopwords_list]\n",
    "        list_c=[stemm.convert_to_stem(x) for x in comment_list if not x in stopwords_list]\n",
    "        title_list=deque(list_t)\n",
    "        comment_list=deque(list_c)\n",
    "        comments.append(comment_list)\n",
    "        titles.append(title_list)\n",
    "        comment_list+=title_list\n",
    "        predict=predict_recom(occurences_recom,occurences_nrecom,repeats_recom,repeats_nrecom,list(comment_list))\n",
    "        if predict==1:\n",
    "            naive_bayes_predicts.append('recommended')\n",
    "        else:\n",
    "            naive_bayes_predicts.append('not_recommended')\n",
    "    return naive_bayes_predicts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed=naive_bayes_preprocessed_smoothed(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpreprocessed and smoothed\n",
    "def predict_recom(word_bag_recom,word_bag_nrecom,repeats_recom,repeats_nrecom,bag):\n",
    "    probab_recom=calc_probab(word_bag_recom,repeats_recom,bag)\n",
    "    probab_nrecom=calc_probab(word_bag_nrecom,repeats_nrecom,bag)\n",
    "    \n",
    "    if probab_recom>=probab_nrecom:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def calc_probab(trained_bag,repeats,bag):\n",
    "    n=0\n",
    "    probab=1\n",
    "    tot=len(trained_bag)\n",
    "    words_probab=[]\n",
    "    for word in bag:\n",
    "        exists=trained_bag.count(word)\n",
    "        if exists:\n",
    "            idx=trained_bag.index(word)\n",
    "            words_probab.append(repeats[idx])\n",
    "        else:\n",
    "            words_probab.append(0.5)\n",
    "            tot=tot+0.5\n",
    "            \n",
    "    words_probab=[x/tot for x in words_probab]\n",
    "    return np.prod(words_probab)\n",
    "\n",
    "def naive_bayes_upreprocessed_smoothed(test):\n",
    "    naive_bayes_predicts=[]\n",
    "    comments=deque([])\n",
    "    titles=deque([])\n",
    "    bag=deque([])\n",
    "    for i in range(test.shape[0]):\n",
    "        title_list=word_tokenize(test.iloc[i,0])\n",
    "        comment_list=word_tokenize(test.iloc[i,1])\n",
    "        comments.append(comment_list)\n",
    "        titles.append(title_list)\n",
    "        comment_list+=title_list\n",
    "        predict=predict_recom(occurences_recom,occurences_nrecom,repeats_recom,repeats_nrecom,list(comment_list))\n",
    "        if predict==1:\n",
    "            naive_bayes_predicts.append('recommended')\n",
    "        else:\n",
    "            naive_bayes_predicts.append('not_recommended')\n",
    "    \n",
    "    return naive_bayes_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unpreprocessed=naive_bayes_upreprocessed_smoothed(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed & unsmoothed\n",
    "def predict_recom(word_bag_recom,word_bag_nrecom,repeats_recom,repeats_nrecom,bag):\n",
    "    probab_recom=calc_probab(word_bag_recom,repeats_recom,bag)\n",
    "    probab_nrecom=calc_probab(word_bag_nrecom,repeats_nrecom,bag)\n",
    "    \n",
    "    if probab_recom>=probab_nrecom:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def calc_probab(trained_bag,repeats,bag):\n",
    "    n=0\n",
    "    probab=1\n",
    "    tot=len(trained_bag)\n",
    "    words_probab=[]\n",
    "    for word in bag:\n",
    "        words_probab.append(trained_bag.count(word))\n",
    "    words_probab=[x/tot for x in words_probab]\n",
    "    return np.prod(words_probab)\n",
    "def naive_bayes_preprocessed_unsmoothed(test):\n",
    "    normalizer=Normalizer()\n",
    "    lemmatizer=Lemmatizer()\n",
    "    stemmer=Stemmer()\n",
    "    stemm=FindStems()\n",
    "    for i in range(test.shape[0]):\n",
    "        for j in range(3):\n",
    "            normalizer.normalize(test.iloc[i,j])\n",
    "    naive_bayes_predicts=[]\n",
    "    comments=deque([])\n",
    "    titles=deque([])\n",
    "    bag=deque([])\n",
    "    for i in range(test.shape[0]):\n",
    "        title_list=word_tokenize(test.iloc[i,0])\n",
    "        comment_list=word_tokenize(test.iloc[i,1])\n",
    "        list_t=[stemm.convert_to_stem(x) for x in title_list if not x in stopwords_list]\n",
    "        list_c=[stemm.convert_to_stem(x) for x in comment_list if not x in stopwords_list]\n",
    "        title_list=deque(list_t)\n",
    "        comment_list=deque(list_c)\n",
    "        comments.append(comment_list)\n",
    "        titles.append(title_list)\n",
    "        comment_list+=title_list\n",
    "        predict=predict_recom(occurences_recom,occurences_nrecom,repeats_recom,repeats_nrecom,list(comment_list))\n",
    "        if predict==1:\n",
    "            naive_bayes_predicts.append('recommended')\n",
    "        else:\n",
    "            naive_bayes_predicts.append('not_recommended')\n",
    "    return naive_bayes_predicts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unsmoothed=naive_bayes_preprocessed_unsmoothed(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpreprocessed & unsmoothed\n",
    "def predict_recom(word_bag_recom,word_bag_nrecom,repeats_recom,repeats_nrecom,bag):\n",
    "    probab_recom=calc_probab(word_bag_recom,repeats_recom,bag)\n",
    "    probab_nrecom=calc_probab(word_bag_nrecom,repeats_nrecom,bag)\n",
    "    \n",
    "    if probab_recom>=probab_nrecom:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def calc_probab(trained_bag,repeats,bag):\n",
    "    n=0\n",
    "    probab=1\n",
    "    tot=len(trained_bag)\n",
    "    words_probab=[]\n",
    "    for word in bag:\n",
    "        words_probab.append(trained_bag.count(word))\n",
    "    words_probab=[x/tot for x in words_probab]\n",
    "    return np.prod(words_probab)\n",
    "def naive_bayes_unpreprocessed_unsmoothed(test):\n",
    "    naive_bayes_predicts=[]\n",
    "    comments=deque([])\n",
    "    titles=deque([])\n",
    "    bag=deque([])\n",
    "    for i in range(test.shape[0]):\n",
    "        title_list=word_tokenize(test.iloc[i,0])\n",
    "        comment_list=word_tokenize(test.iloc[i,1])\n",
    "        comments.append(comment_list)\n",
    "        titles.append(title_list)\n",
    "        comment_list+=title_list\n",
    "        predict=predict_recom(occurences_recom,occurences_nrecom,repeats_recom,repeats_nrecom,list(comment_list))\n",
    "        if predict==1:\n",
    "            naive_bayes_predicts.append('recommended')\n",
    "        else:\n",
    "            naive_bayes_predicts.append('not_recommended')\n",
    "    \n",
    "    return naive_bayes_predicts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unpreprocessed_unsmoothed=naive_bayes_unpreprocessed_unsmoothed(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_characteristics(output):\n",
    "    recommended_true=0\n",
    "    #accuracy\n",
    "    comparing=0\n",
    "    for i in range(test.shape[0]):\n",
    "        if test['recommend'][i]==output[i]:\n",
    "            comparing=comparing+1\n",
    "            if test['recommend'][i]=='recommended':\n",
    "                recommended_true=recommended_true+1\n",
    "    accuracy=comparing/test.shape[0]\n",
    "    #print(\"ACCURACY : {}\".format(comparing/test.shape[0]))\n",
    "\n",
    "    #precision\n",
    "    recommended_bayes=len([x for x in output if x=='recommended'])\n",
    "    recommended_real=test[test['recommend']=='recommended'].shape[0]\n",
    "    precision=recommended_true/recommended_bayes\n",
    "    #print(\"\\nPRECISION : {}\".format(precision))\n",
    "\n",
    "    #recall\n",
    "    recall=recommended_true/recommended_real\n",
    "    #print(\"\\nRECALL : {}\".format(recall))\n",
    "\n",
    "    #F1\n",
    "    f1=2*(precision*recall)/(precision+recall)\n",
    "    #print(\"\\nF1 : {}\".format(f1)\n",
    "    return [accuracy,precision,recall,f1]\n",
    "out1=calc_characteristics(test_preprocessed)\n",
    "out2=calc_characteristics(test_unpreprocessed)\n",
    "out3=calc_characteristics(test_unsmoothed)\n",
    "out4=calc_characteristics(test_unpreprocessed_unsmoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Preprocessed and Smoothed</th>\n",
       "      <th>Not Preproprecessed and Smoothed</th>\n",
       "      <th>Preprocessed and Not Smoothed</th>\n",
       "      <th>Not Preprocessed and Not Smoothed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.933750</td>\n",
       "      <td>0.857500</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.934837</td>\n",
       "      <td>0.850490</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.932500</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.933667</td>\n",
       "      <td>0.858911</td>\n",
       "      <td>0.748072</td>\n",
       "      <td>0.663866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Preprocessed and Smoothed  Not Preproprecessed and Smoothed  \\\n",
       "0   Accuracy                   0.933750                          0.857500   \n",
       "1  Precision                   0.934837                          0.850490   \n",
       "2     Recall                   0.932500                          0.867500   \n",
       "3         F1                   0.933667                          0.858911   \n",
       "\n",
       "   Preprocessed and Not Smoothed  Not Preprocessed and Not Smoothed  \n",
       "0                       0.755000                           0.500000  \n",
       "1                       0.769841                           0.500000  \n",
       "2                       0.727500                           0.987500  \n",
       "3                       0.748072                           0.663866  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'': ['Accuracy','Precision','Recall','F1'],\n",
    "        'Preprocessed and Smoothed':  out1,\n",
    "        'Not Preproprecessed and Smoothed': out2,\n",
    "        'Preprocessed and Not Smoothed': out3,\n",
    "        'Not Preprocessed and Not Smoothed': out4\n",
    "        }\n",
    "df = pd.DataFrame (data, columns = ['','Preprocessed and Smoothed','Not Preproprecessed and Smoothed','Preprocessed and Not Smoothed','Not Preprocessed and Not Smoothed'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font color='green'>Analysis </br> </font>\n",
    "<br><font color='orange'>We see that preprocessing alone has considerably raised the percentage of accuracy of our classification. Smoothing the values has also had a great impact on the accuracy which is inferered to affect the accuracy more than preprocessing does. With omitting both of the actions we get a very low accuracy and precision which is not desired at all.</br></font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook AI_CA3_810098018.ipynb to html\n",
      "[NbConvertApp] Writing 346437 bytes to AI_CA3_810098018.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html AI_CA3_810098018.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
